@article{bengio2009learning,
  title={Learning deep architectures for {AI}},
  author={Bengio, Yoshua},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers, Inc.}
},

@article{lim2021tensors,
  title={Tensors in computations},
  author={Lim, Lek-Heng},
  journal={Acta Numerica},
  volume={30},
  pages={555--764},
  year={2021},
  publisher={Cambridge University Press}
},

@book{anderson2000talking,
  title={Talking nets: An oral history of neural networks},
  author={Anderson, James A and Rosenfeld, Edward},
  year={2000},
  publisher={MIT Press}
},

@book{metz2022genius,
  title={Genius makers: the mavericks who brought AI to Google, Facebook, and the world},
  author={Metz, Cade},
  year={2022},
  publisher={Penguin}
},

@book{prince2023understanding,
  title={Understanding Deep Learning},
  author={Prince, Simon JD},
  year={2023},
  publisher={MIT Press}
},

@book{zhang2023dive,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year={2023},
  publisher={Cambridge University Press}
},

@book{bishop2024deep,
  title={Deep learning: Foundations and concepts},
  author={Bishop, Christopher M and Bishop, Hugh},
  year={2023},
  publisher={Springer Nature}
},

@book{fleuret2023little,
  title={The Little Book of Deep Learning},
  author={Fleuret, Fran{\c{c}}ois},
  publisher={Lulu Press, Inc.},
  year={2023}
},

@book{hardt2022patterns,
  title={Patterns, predictions, and actions: Foundations of machine learning},
  author={Hardt, Moritz and Recht, Benjamin},
  year={2022},
  publisher={Princeton University Press}
},

@article{wang2023scientific,
  title={Scientific discovery in the age of artificial intelligence},
  author={Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={47--60},
  year={2023},
  publisher={Nature Portfolio}
},

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  year={2022}
},

@incollection{rumelhart1986general,
  title={A general framework for parallel distributed processing},
  author={Rumelhart, David E and Hinton, Geoffrey E and McClelland, James L},
  booktitle={Parallel Distributed Processing Volume 1},
  pages={26},
  year={1986},
  publisher={MIT Press}
},

@article{zador2023catalyzing,
  title={Catalyzing next-generation artificial intelligence through {neuroAI}},
  author={Zador, Anthony and Escola, Sean and Richards, Blake and {\"O}lveczky, Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and others},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={1597},
  year={2023},
  publisher={Nature Portfolio}
},

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Portfolio}
},

@article{bronstein2017geometric,
  title={Geometric deep learning: going beyond {Euclidean} data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
},

@article{stigler1981gauss,
  title={Gauss and the invention of least squares},
  author={Stigler, Stephen M},
  journal={The Annals of Statistics},
  pages={465--474},
  year={1981},
  publisher={JSTOR}
},

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
},

@article{al2016theano,
  title={Theano: A Python framework for fast computation of mathematical expressions},
  author={Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'e}d{\'e}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and others},
  journal={arXiv preprint arXiv:1605.02688},
  pages= {1-19},
  year={2016}
},

@book{petersen2008matrix,
  title={The matrix cookbook},
  author={Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  publisher={Technical University of Denmark},
  year={2008}
},

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR},
  year={2015}
},

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher},
  year={2006},
  publisher={Springer}
},

@book{bogachev2007measure,
  title={Measure theory},
  author={Bogachev, Vladimir Igorevich and Ruas, Maria Aparecida Soares},
  year={2007},
  publisher={Springer}
},

@inproceedings{cinus2022effect,
  title={The effect of people recommenders on echo chambers and polarization},
  author={Cinus, Federico and Minici, Marco and Monti, Corrado and Bonchi, Francesco},
  booktitle={AAAI ICWSM},
  volume={16},
  pages={90--101},
  year={2022}
},

@inproceedings{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={ICML},
  pages={12310--12320},
  year={2021},
  organization={PMLR}
},

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
},

@article{parisi2019continual,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural Networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
},

@inproceedings{biesialska2020continual,
  title={Continual lifelong learning in natural language processing: A survey},
  author={Biesialska, Magdalena and Biesialska, Katarzyna and Costa-Jussa, Marta R},
  booktitle={COLING},
  pages={6523-6541},
  year={2020}
},

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={ICLR},
  year={2023}
},

@inproceedings{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  booktitle={1st Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR},
  year={2022}
},

@article{jospin2022hands,
  title={Hands-on Bayesian neural networks—A tutorial for deep learning users},
  author={Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Buntine, Wray and Bennamoun, Mohammed},
  journal={IEEE Computational Intelligence Magazine},
  volume={17},
  number={2},
  pages={29--48},
  year={2022},
  publisher={IEEE}
},

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
},

@inproceedings{blasiok2024does,
  title={When Does Optimizing a Proper Loss Yield Calibration?},
  author={Blasiok, Jaroslaw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
  booktitle={NeurIPS},
  year={2024}
},

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={ICML},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
},

@inproceedings{mukhoti2020calibrating,
  title={Calibrating deep neural networks using focal loss},
  author={Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip and Dokania, Puneet},
  booktitle={NeurIPS},
  pages={15288--15299},
  year={2020}
},

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
},

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural Networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
},

@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={NeurIPS},
  year={2017}
},

@inproceedings{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={ICLR},
  year={2019}
},

@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={NeurIPS},
  year={2019}
},

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  year={2012}
},

@article{baydin2018automatic,
  title={Automatic differentiation in machine learning: a survey},
  author={Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  journal={Journal of Marchine Learning Research},
  volume={18},
  pages={1--43},
  year={2018},
  publisher={Microtome Publishing}
},

@article{laue2019equivalence,
  title={On the Equivalence of Automatic and Symbolic Differentiation},
  author={Laue, S{\"o}ren},
  journal={arXiv preprint arXiv:1904.02990},
  year={2019}
},

@book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  year={2008},
  publisher={SIAM}
},

@inproceedings{bolte2020mathematical,
  title={A mathematical model for automatic differentiation in machine learning},
  author={Bolte, J{\'e}r{\^o}me and Pauwels, Edouard},
  booktitle={NeurIPS},
  pages={10809--10819},
  year={2020}
},

@inproceedings{kakade2018provably,
  title={Provably correct automatic sub-differentiation for qualified programs},
  author={Kakade, Sham M and Lee, Jason D},
  booktitle={NeurIPS},
  year={2018}
},

@inproceedings{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  booktitle={NeurIPS},
  pages={507--520},
  year={2022}
},

@book{uncini2015fundamentals,
  title={Fundamentals of adaptive signal processing},
  author={Uncini, Aurelio},
  year={2015},
  publisher={Springer}
},

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={IEEE CVPR},
  pages={1--9},
  year={2015}
},

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
},

@article{shibata1999byte,
  title={Byte Pair encoding: A text compression scheme that accelerates pattern matching},
  author={Shibata, Yusuxke and Kida, Takuya and Fukamachi, Shuichi and Takeda, Masayuki and Shinohara, Ayumi and Shinohara, Takeshi and Arikawa, Setsuo},
  year={1999},
  publisher={Technical Report DOI-TR-161, Department of Informatics, Kyushu University}
},

@inproceedings{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  booktitle={ISCA SSW Workshop},
  year={2016}
},

@inproceedings{loshchilov2018fixing,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2019}
},

@article{scardapane2017group,
  title={Group sparse regularization for deep neural networks},
  author={Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  journal={Neurocomputing},
  volume={241},
  pages={81--89},
  year={2017},
  publisher={Elsevier}
},

@article{bach2012optimization,
  title={Optimization with sparsity-inducing penalties},
  author={Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={1},
  pages={1--106},
  year={2012},
  publisher={Now Publishers, Inc.}
},

@article{bishop1995training,
  title={Training with noise is equivalent to Tikhonov regularization},
  author={Bishop, Chris M},
  journal={Neural Computation},
  volume={7},
  number={1},
  pages={108--116},
  year={1995},
  publisher={MIT Press}
},

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={IEEE/CVF CVPR Workshops},
  pages={702--703},
  year={2020}
},

@inproceedings{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={ICLR},
  year={2017}
},

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={IEEE/CVF ICCV},
  pages={6023--6032},
  year={2019}
},

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={Microtome Publishing}
},

@inproceedings{gal2016dropout,
  title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={ICML},
  pages={1050--1059},
  year={2016},
},

@inproceedings{tompson2015efficient,
  title={Efficient object localization using convolutional networks},
  author={Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
  booktitle={IEEE/CVF CVPR},
  pages={648--656},
  year={2015}
},

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
},

@inproceedings{wan2013regularization,
  title={Regularization of neural networks using {DropConnect}},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle={ICML},
  pages={1058--1066},
  year={2013},
},

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  pages={448--456},
  year={2015},
},

@inproceedings{bjorck2018understanding,
  title={Understanding batch normalization},
  author={Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  booktitle={NeurIPS},
  year={2018}
},

@article{wu2021rethinking,
  title={Rethinking "{B}atch" in {BatchNorm}},
  author={Wu, Yuxin and Johnson, Justin},
  journal={arXiv preprint arXiv:2105.07576},
  year={2021}
},

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
},

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE/CVF CVPR},
  pages={770--778},
  year={2016}
},

@inproceedings{de2020batch,
  title={Batch normalization biases residual blocks towards the identity function in deep networks},
  author={De, Soham and Smith, Sam},
  booktitle={NeurIPS},
  pages={19964--19975},
  year={2020}
},

@inproceedings{liu2022convnet,
  title={A {ConvNet} for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={IEEE/CVF CVPR},
  pages={11976--11986},
  year={2022}
},

@inproceedings{veit2016residual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  booktitle={NeurIPS},
  year={2016}
},

@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={NeurIPS},
  year={2018}
},

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
},

@inproceedings{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={ICLR},
  year={2015}
},

@article{romero2022towards,
  title={Towards a General Purpose CNN for Long Range Dependencies in $N$D},
  author={Romero, David W and Knigge, David M and Gu, Albert and Bekkers, Erik J and Gavves, Efstratios and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2206.03398},
  year={2022}
},

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={IEEE/CVF CVPR},
  pages={7794--7803},
  year={2018}
},

@inproceedings{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  booktitle={ICLR},
  year={2022}
},

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
},

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={ICML},
  pages={10524--10533},
  year={2020},
},

@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2021}
},

@inproceedings{darcet2023vision,
  title={Vision transformers need registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle={ICLR},
  year={2024}
},

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={NeurIPS},
  year={2014}
},

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
},

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={ICML},
  pages={19274--19286},
  year={2023},
},

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={ICML},
  pages={4651--4664},
  year={2021},
},

@article{rabe2021self,
  title={Self-attention Does Not Need $\mathcal{O}(n^{2})$ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
},

@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  pages={4171–4186},
  year={2018},
  publisher={ACL}
},

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={Microtome Publishing}
},

@inproceedings{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  booktitle={NeurIPS},
  pages={12449--12460},
  year={2020}
},

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={ICML},
  pages={7480--7512},
  year={2023},
},

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
},

@inproceedings{tolstikhin2021mlp,
  title={{MLP}-mixer: An all-{MLP} architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  booktitle={NeurIPS},
  pages={24261--24272},
  year={2021}
},

@inproceedings{yu2022s2,
  title={S2-{MLP}: Spatial-shift {MLP} architecture for vision},
  author={Yu, Tan and Li, Xu and Cai, Yunfeng and Sun, Mingming and Li, Ping},
  booktitle={IEEE/CVF WACV},
  pages={297--306},
  year={2022}
},

@inproceedings{yu2022metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={IEEE/CVF CVPR},
  pages={10819--10829},
  year={2022}
},

@inproceedings{10.5555/3305381.3305478,
    author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
    title = {Language modeling with gated convolutional networks},
    year = {2017},
    booktitle = {ICML},
    pages = {933–941},
},

@inproceedings{liu2021pay,
  title={Pay attention to {MLP}s},
  author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
  booktitle={NeurIPS},
  pages={9204--9215},
  year={2021}
},

@inproceedings{cheng2024multilinear,
  title={Multilinear Operator Networks},
  author={Cheng, Yixin and Chrysos, Grigorios G and Georgopoulos, Markos and Cevher, Volkan},
  booktitle={ICLR},
  year={2024}
},

@article{belkin2006manifold,
  title={Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.},
  author={Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={11},
  year={2006}
},

@inproceedings{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={ICLR},
  year={2017}
},

@article{grattarola2022understanding,
  title={Understanding pooling in graph neural networks},
  author={Grattarola, Daniele and Zambon, Daniele and Bianchi, Filippo Maria and Alippi, Cesare},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
},

@inproceedings{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  booktitle={NeurIPS},
  year={2017}
},

@article{li2022graph,
  title={Graph Representation Learning Beyond Node and Homophily},
  author={Li, You and Lin, Bei and Luo, Binli and Gui, Ning},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={5},
  pages={4880--4893},
  year={2022},
  publisher={IEEE}
},

@inproceedings{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  booktitle={ICLR},
  year={2018}
},

@inproceedings{brody2021attentive,
  title={How attentive are graph attention networks?},
  author={Brody, Shaked and Alon, Uri and Yahav, Eran},
  booktitle={ICLR},
  year={2022}
},

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={ICML},
  pages={1263--1272},
  year={2017},
},

@article{velivckovic2022message,
  title={Message passing all the way up},
  author={Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2202.11097},
  year={2022}
},

@inproceedings{morris2019weisfeiler,
  title={Weisfeiler and leman go neural: Higher-order graph neural networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={4602--4609},
  year={2019}
},

@inproceedings{yu2017spatio,
  title={Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting},
  author={Yu, Bing and Yin, Haoteng and Zhu, Zhanxing},
  booktitle={IJCAI},
  year={2017}
},

@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
},

@inproceedings{chien2021you,
  title={You are allset: A multiset function framework for hypergraph neural networks},
  author={Chien, Eli and Pan, Chao and Peng, Jianhao and Milenkovic, Olgica},
  booktitle={ICLR},
  year={2022}
},

@inproceedings{satorras2021n,
  title={E(n) equivariant graph neural networks},
  author={Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={ICML},
  pages={9323--9332},
  year={2021},
},

@inproceedings{eijkelboom2023n,
  title={E $(n) $ Equivariant Message Passing Simplicial Networks},
  author={Eijkelboom, Floor and Hesselink, Rob and Bekkers, Erik J},
  booktitle={ICML},
  pages={9071--9081},
  year={2023},
}

@article{muller2023attending,
  title={Attending to graph transformers},
  author={M{\"u}ller, Luis and Galkin, Mikhail and Morris, Christopher and Ramp{\'a}{\v{s}}ek, Ladislav},
  journal={Transactions on Machine Learning Research},
  year={2024}
},

@inproceedings{rampavsek2022recipe,
  title={Recipe for a general, powerful, scalable graph transformer},
  author={Ramp{\'a}{\v{s}}ek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  booktitle={NeurIPS},
  pages={14501--14515},
  year={2022}
},

@inproceedings{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  booktitle={NeurIPS},
  pages={1950--1965},
  year={2022}
},

@inproceedings{dwivedi2021graph,
  title={Graph neural networks with learnable structural and positional representations},
  author={Dwivedi, Vijay Prakash and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  booktitle={ICLR},
  year={2022}
},

@inproceedings{lim2022sign,
  title={Sign and basis invariant networks for spectral graph representation learning},
  author={Lim, Derek and Robinson, Joshua and Zhao, Lingxiao and Smidt, Tess and Sra, Suvrit and Maron, Haggai and Jegelka, Stefanie},
  booktitle={ICLR},
  year={2023}
},

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={ICML},
  pages={5156--5165},
  year={2020},
},

@article{hofmann2008kernel,
  title={Kernel methods in machine learning},
  author={Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  journal={The Annals of Statistics},
  volume={36},
  number={3},
  pages={1171--1220},
  year={2008}
},

@article{scardapane2017randomness,
  title={Randomness in neural networks: an overview},
  author={Scardapane, Simone and Wang, Dianhui},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={7},
  number={2},
  pages={e1200},
  year={2017},
  publisher={Wiley Online Library}
},

@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE Transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={IEEE}
},

@inproceedings{pascanu2013construct,
  title={How to construct deep recurrent neural networks},
  author={Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={ICLR},
  year={2014}
},

@article{lukovsevivcius2009reservoir,
  title={Reservoir computing approaches to recurrent neural network training},
  author={Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  journal={Computer Science Review},
  volume={3},
  number={3},
  pages={127--149},
  year={2009},
  publisher={Elsevier}
},

@article{gauthier2021next,
  title={Next generation reservoir computing},
  author={Gauthier, Daniel J and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson AS},
  journal={Nature Communications},
  volume={12},
  number={1},
  pages={5564},
  year={2021},
  publisher={Nature Portfolio}
},

@article{hochreiter1998recurrent,
  title={Recurrent neural net learning and vanishing gradient},
  author={Hochreiter, Sepp},
  journal={International Journal Of Uncertainity, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={2},
  pages={107--116},
  year={1998}
},

@article{ravanelli2018light,
  title={Light gated recurrent units for speech recognition},
  author={Ravanelli, Mirco and Brakel, Philemon and Omologo, Maurizio and Bengio, Yoshua},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={2},
  number={2},
  pages={92--102},
  year={2018},
  publisher={IEEE}
},

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
},

@inproceedings{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={EMNLP},
  year={2014},
  publisher={ACL}
},

@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
},

@article{zhai2021attention,
  title={An attention free transformer},
  author={Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
  journal={arXiv preprint arXiv:2105.14103},
  year={2021}
},

@inproceedings{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  booktitle={EMNLP},
  year={2023},
  publisher={ACL}
},

@inproceedings{fu2022hungry,
  title={Hungry hungry hippos: Towards language modeling with state space models},
  author={Fu, Daniel Y and Dao, Tri and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  booktitle={ICLR},
  year={2023}
},

@inproceedings{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle={ICML},
  year={2023}
},

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
},

@article{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  pages={572--585},
  year={2021}
},

@inproceedings{orvieto2023universality,
  title={On the Universality of Linear Recurrences Followed by Nonlinear Projections},
  author={Orvieto, Antonio and De, Soham and Gulcehre, Caglar and Pascanu, Razvan and Smith, Samuel L},
  booktitle={HLD 2023 Workshop, ICML},
  year={2023}
},

@inproceedings{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  pages={1474--1487},
  year={2020}
},

@inproceedings{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={ICLR},
  year={2022}
},

@inproceedings{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  booktitle={ICLR},
  year={2023}
},

@inproceedings{orvieto2023resurrecting,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={ICML},
  year={2023}
},

@book{blelloch1990prefix,
  title={Prefix sums and their applications},
  author={Blelloch, Guy E},
  year={1990},
  publisher={School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA}
},

@article{jain2017non,
  title={Non-convex optimization for machine learning},
  author={Jain, Prateek and Kar, Purushottam and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={10},
  number={3-4},
  pages={142--363},
  year={2017},
  publisher={Now Publishers, Inc.}
},

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={ICML},
  pages={1139--1147},
  year={2013},
},

@inproceedings{yuan2024revisiting,
  title={Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations},
  author={Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  booktitle={NeurIPS},
  year={2024}
},

@inproceedings{huang2014deep,
  title={Deep embedding network for clustering},
  author={Huang, Peihao and Huang, Yan and Wang, Wei and Wang, Liang},
  booktitle={ICPR},
  pages={1532--1537},
  year={2014},
  organization={IEEE}
},

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={NeurIPS},
  year={2013}
},

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer Science \& Business Media}
},

@article{poggio2003mathematics,
  title={The mathematics of learning: Dealing with data},
  author={Poggio, Tomaso and Smale, Steve and others},
  journal={Notices of the AMS},
  volume={50},
  number={5},
  pages={537--544},
  year={2003}
},

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
},

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT Press}
},

@article{poggio2020theoretical,
  title={Theoretical issues in deep networks},
  author={Poggio, Tomaso and Banburski, Andrzej and Liao, Qianli},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30039--30045},
  year={2020},
  publisher={National Academy of Sciences}
},

@inproceedings{irie2022dual,
  title={The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention},
  author={Irie, Kazuki and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  booktitle={ICML},
  pages={9639--9659},
  year={2022},
},

@article{coppersmith1982asymptotic,
  title={On the asymptotic complexity of matrix multiplication},
  author={Coppersmith, Don and Winograd, Shmuel},
  journal={SIAM Journal on Computing},
  volume={11},
  number={3},
  pages={472--492},
  year={1982},
  publisher={SIAM}
},

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE ICCV},
  pages={1026--1034},
  year={2015}
},

@article{hendrycks2016gaussian,
  title={Gaussian error linear units ({GELUs})},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
},

@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
},

@article{ho2024algorithmic,
      title={Algorithmic progress in language models}, 
      author={Anson Ho and Tamay Besiroglu and Ege Erdil and David Owen and Robi Rahman and Zifan Carl Guo and David Atkinson and Neil Thompson and Jaime Sevilla},
      year={2024},
      journal={arXiv preprint arXiv:1710.05941}
},

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
},

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM}
},

@inproceedings{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  booktitle={ICLR},
  year={2023}
},

@inproceedings{ziyin2023spred,
  title={spred: Solving L1 Penalty with SGD},
  author={Ziyin, Liu and Wang, Zihao},
  booktitle={ICML},
  pages={43407--43422},
  year={2023},
},

@article{rocks2022memorizing,
  title={Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical Review Research},
  volume={4},
  number={1},
  pages={013201},
  year={2022},
  publisher={APS}
},

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
},

@article{blondel2024elements,
      title={The Elements of Differentiable Programming}, 
      author={Mathieu Blondel and Vincent Roulet},
      year={2024},
      journal={arXiv preprint  arXiv:2403.14606},
},

@article{mialon2023augmented,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
},

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
},

@inproceedings{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  booktitle={NeurIPS},
  volume={29},
  year={2016}
},

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={ACM FAccT},
  pages={610--623},
  year={2021},
  publisher={ACM}
},

@article{hackenberger2019bayes,
  title={Bayes or not Bayes, is this the question?},
  author={Hackenberger, Branimir K},
  journal={Croatian Medical Journal},
  volume={60},
  number={1},
  pages={50},
  year={2019},
  publisher={Medicinska Naklada}
},

@article{angelopoulos2021gentle,
  title={A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
  author={Angelopoulos, Anastasios N and Bates, Stephen},
  journal={arXiv preprint arXiv:2107.07511},
  year={2021}
},

@article{scardapane2019kafnets,
  title={Kafnets: Kernel-based non-parametric activation functions for neural networks},
  author={Scardapane, Simone and Van Vaerenbergh, Steven and Totaro, Simone and Uncini, Aurelio},
  journal={Neural Networks},
  volume={110},
  pages={19--32},
  year={2019},
  publisher={Elsevier}
},

@article{apicella2021survey,
  title={A survey on modern trainable activation functions},
  author={Apicella, Andrea and Donnarumma, Francesco and Isgr{\`o}, Francesco and Prevete, Roberto},
  journal={Neural Networks},
  volume={138},
  pages={14--32},
  year={2021},
  publisher={Elsevier}
},

@article{agostinelli2014learning,
  title={Learning activation functions to improve deep neural networks},
  author={Agostinelli, Forest and Hoffman, Matthew and Sadowski, Peter and Baldi, Pierre},
  journal={arXiv preprint arXiv:1412.6830},
  year={2014}
},

@article{marra2018learning,
  title={Learning neuron non-linearities with kernel-based deep neural networks},
  author={Marra, Giuseppe and Zanca, Dario and Betti, Alessandro and Gori, Marco},
  journal={arXiv preprint arXiv:1807.06302},
  year={2018}
},

@article{li2023generalized,
  title={Generalized Activation via Multivariate Projection},
  author={Li, Jiayun and Cheng, Yuxiao and Xia, Zhuofan and Mo, Yilin and Huang, Gao},
  journal={arXiv preprint arXiv:2309.17194},
  year={2023}
},

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={ICML},
  pages={933--941},
  year={2017},
},

@article{shazeer2020glu,
  title={{GLU} variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
},

@inproceedings{goodfellow2013maxout,
  title={Maxout networks},
  author={Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  booktitle={ICML},
  pages={1319--1327},
  year={2013},
},

@inproceedings{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  booktitle={NeurIPS},
  year={2019}
},

@article{griewank2012invented,
  title={Who invented the reverse mode of differentiation?},
  author={Griewank, Andreas},
  journal={Documenta Mathematica, Extra Volume ISMP},
  volume={389400},
  year={2012}
},

@article{puny2021frame,
  title={Frame averaging for invariant and equivariant network design},
  author={Puny, Omri and Atzmon, Matan and Ben-Hamu, Heli and Misra, Ishan and Grover, Aditya and Smith, Edward J and Lipman, Yaron},
  journal={arXiv preprint arXiv:2110.03336},
  year={2021}
},

@article{golkar2023xval,
  title={xval: A continuous number encoding for large language models},
  author={Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others},
  journal={arXiv preprint arXiv:2310.02989},
  year={2023}
},

@article{ansari2024chronos,
  title={Chronos: Learning the Language of Time Series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
},

@inproceedings{patel2024datadreamer,
  title={DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows},
  author={Patel, Ajay and Raffel, Colin and Callison-Burch, Chris},
  booktitle={ACL},
  year={2024},
  publisher={ACL}
},

@inproceedings{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  booktitle={NeurIPS},
  year={2019}
},

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  pages={16344--16359},
  year={2022}
},

@inproceedings{chang2022maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={IEEE/CVF CVPR},
  pages={11315--11325},
  year={2022}
},

@article{tschannen2023givt,
  title={GIVT: Generative Infinite-Vocabulary Transformers},
  author={Tschannen, Michael and Eastwood, Cian and Mentzer, Fabian},
  journal={arXiv preprint arXiv:2312.02116},
  year={2023}
},

@inproceedings{graves2012connectionist,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={ICML},
  pages={369--376},
  year={2006}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={ICML},
  pages={28492--28518},
  year={2023},
},

@article{defossez2022high,
  title={High fidelity neural audio compression},
  author={D{\'e}fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
  journal={Transactions on Machine Learning Research},
  year={2023}
},

@article{wang2023neural,
  title={Neural codec language models are zero-shot text to speech synthesizers},
  author={Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal={arXiv preprint arXiv:2301.02111},
  year={2023}
},

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle={NeurIPS},
  year={2015}
},

@inproceedings{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle={EMNLP},
  pages={5484–5495},
  year={2020},
  publisher={ACL}
},

@inproceedings{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={ICML},
  pages={9099--9117},
  year={2022},
},

@article{bianchi2021graph,
  title={Graph neural networks with convolutional arma filters},
  author={Bianchi, Filippo Maria and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={7},
  pages={3496--3507},
  year={2021},
  publisher={IEEE}
},

@inproceedings{gori2005new,
  title={A new model for learning in graph domains},
  author={Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
  booktitle={IEEE IJCNN},
  volume={2},
  pages={729--734},
  year={2005},
  organization={IEEE}
}

@article{scarselli2008graph,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
},

@article{bordes2024introduction,
  title={An Introduction to Vision-Language Modeling},
  author={Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C and Bardes, Adrien and Petryk, Suzanne and Ma{\~n}as, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and others},
  journal={arXiv preprint arXiv:2405.17247},
  year={2024}
},

@article{chung2021turing,
  title={Turing completeness of bounded-precision recurrent neural networks},
  author={Chung, Stephen and Siegelmann, Hava},
  booktitle={NeurIPS},
  pages={28431--28441},
  year={2021}
},

@inproceedings{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  booktitle={Foundation Models for Decision Making Workshop, NeurIPS},
  year={2023}
}

% Explainability

@inproceedings{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={International Conference on Machine Learning},
  pages={3319--3328},
  year={2017},
  organization={PMLR}
},

@article{bilodeau2024impossibility,
  title={Impossibility theorems for feature attribution},
  author={Bilodeau, Blair and Jaques, Natasha and Koh, Pang Wei and Kim, Been},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={2},
  pages={e2304406120},
  year={2024},
  publisher={National Acad Sciences}
},

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Portfolio}
},

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
},

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
},

@article{pesme2021implicit,
  title={Implicit bias of {SGD} for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  booktitle={NeurIPS},
  volume={34},
  pages={29218--29230},
  year={2021}
},

@inproceedings{teney2024neural,
  title={Neural Redshift: Random Networks are not Random Functions},
  author={Teney, Damien and Nicolicioiu, Armand Mihai and Hartmann, Valentin and Abbasnejad, Ehsan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4786--4796},
  year={2024}
},

@article{liu2024kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
},

@article{kidger2021equinox,
    author={Patrick Kidger and Cristian Garcia},
    title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},
    year={2021},
    journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}
},

@article{niculae2023discrete,
  title={Discrete latent structure in neural networks},
  author={Niculae, Vlad and Corro, Caio F and Nangia, Nikita and Mihaylova, Tsvetomila and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2301.07473},
  year={2023}
},

@article{qiu2024compute,
  title={Compute Better Spent: Replacing Dense Layers with Structured Matrices},
  author={Qiu, Shikai and Potapczynski, Andres and Finzi, Marc and Goldblum, Micah and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2406.06248},
  year={2024}
},

@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
},

@article{papamakarios2021normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
},

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  booktitle={NeurIPS},
  volume={30},
  year={2017}
},

@article{lee2021vision,
  title={Vision transformer for small-size datasets},
  author={Lee, Seung Hoon and Lee, Seunghyun and Song, Byung Cheol},
  journal={arXiv preprint arXiv:2112.13492},
  year={2021}
},

@article{steiner2021train,
  title={How to train your vit? data, augmentation, and regularization in vision transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
},

@inproceedings{maoposition,
  title={Position: Graph Foundation Models Are Already Here},
  author={Mao, Haitao and Chen, Zhikai and Tang, Wenzhuo and Zhao, Jianan and Ma, Yao and Zhao, Tong and Shah, Neil and Galkin, Mikhail and Tang, Jiliang},
  booktitle={Forty-first International Conference on Machine Learning}
},

@article{tiezzi2024state,
  title={State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era},
  author={Tiezzi, Matteo and Casoni, Michele and Betti, Alessandro and Gori, Marco and Melacci, Stefano},
  journal={arXiv preprint arXiv:2406.09062},
  year={2024}
},

@article{gallicchio2017echo,
  title={Echo state property of deep reservoir computing networks},
  author={Gallicchio, Claudio and Micheli, Alessio},
  journal={Cognitive Computation},
  volume={9},
  pages={337--350},
  year={2017},
  publisher={Springer}
},

@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}